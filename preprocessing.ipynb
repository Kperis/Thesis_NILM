{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done extracting Korean data\n"
     ]
    }
   ],
   "source": [
    "#extract korean dataset\n",
    "from load_korean_data import extract_zips\n",
    "\n",
    "extract_zips()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Room</th>\n",
       "      <th>CO2[ppm]</th>\n",
       "      <th>PM4[ug/m3]</th>\n",
       "      <th>Lighting[lux]</th>\n",
       "      <th>T_in[°C]</th>\n",
       "      <th>RH [%]</th>\n",
       "      <th>PM10[ug/m3]</th>\n",
       "      <th>PM2_5[ug/m3]</th>\n",
       "      <th>PM1[ug/m3]</th>\n",
       "      <th>PM0_5[ug/m3]</th>\n",
       "      <th>T_out [°C]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-07-01 00:00:00</td>\n",
       "      <td>E145</td>\n",
       "      <td>624.23</td>\n",
       "      <td>65.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.46</td>\n",
       "      <td>55.51</td>\n",
       "      <td>65.44</td>\n",
       "      <td>65.42</td>\n",
       "      <td>65.22</td>\n",
       "      <td>56.81</td>\n",
       "      <td>24.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-07-01 01:00:00</td>\n",
       "      <td>E145</td>\n",
       "      <td>629.00</td>\n",
       "      <td>66.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.48</td>\n",
       "      <td>56.39</td>\n",
       "      <td>66.78</td>\n",
       "      <td>66.76</td>\n",
       "      <td>66.56</td>\n",
       "      <td>57.97</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-07-01 02:00:00</td>\n",
       "      <td>E145</td>\n",
       "      <td>640.13</td>\n",
       "      <td>67.78</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.45</td>\n",
       "      <td>56.74</td>\n",
       "      <td>67.79</td>\n",
       "      <td>67.77</td>\n",
       "      <td>67.57</td>\n",
       "      <td>58.85</td>\n",
       "      <td>22.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-07-01 03:00:00</td>\n",
       "      <td>E145</td>\n",
       "      <td>639.77</td>\n",
       "      <td>68.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.39</td>\n",
       "      <td>57.08</td>\n",
       "      <td>68.01</td>\n",
       "      <td>67.98</td>\n",
       "      <td>67.78</td>\n",
       "      <td>59.04</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-07-01 04:00:00</td>\n",
       "      <td>E145</td>\n",
       "      <td>643.31</td>\n",
       "      <td>67.25</td>\n",
       "      <td>0.03</td>\n",
       "      <td>28.35</td>\n",
       "      <td>57.57</td>\n",
       "      <td>67.26</td>\n",
       "      <td>67.24</td>\n",
       "      <td>67.04</td>\n",
       "      <td>58.39</td>\n",
       "      <td>21.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  Room  CO2[ppm]  PM4[ug/m3]  Lighting[lux]  T_in[°C]  \\\n",
       "0  2024-07-01 00:00:00  E145    624.23       65.43           0.00     28.46   \n",
       "1  2024-07-01 01:00:00  E145    629.00       66.77           0.00     28.48   \n",
       "2  2024-07-01 02:00:00  E145    640.13       67.78           0.00     28.45   \n",
       "3  2024-07-01 03:00:00  E145    639.77       68.00           0.00     28.39   \n",
       "4  2024-07-01 04:00:00  E145    643.31       67.25           0.03     28.35   \n",
       "\n",
       "   RH [%]  PM10[ug/m3]  PM2_5[ug/m3]  PM1[ug/m3]  PM0_5[ug/m3]  T_out [°C]  \n",
       "0   55.51        65.44         65.42       65.22         56.81        24.5  \n",
       "1   56.39        66.78         66.76       66.56         57.97        24.0  \n",
       "2   56.74        67.79         67.77       67.57         58.85        22.3  \n",
       "3   57.08        68.01         67.98       67.78         59.04        22.0  \n",
       "4   57.57        67.26         67.24       67.04         58.39        21.5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_madalena_comfort = pd.read_csv('datasets/madalena_comfort.csv')\n",
    "\n",
    "df_madalena_comfort.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import gc\n",
    "\n",
    "\n",
    "def optimize_memory(df):\n",
    "\n",
    "    for col in df.select_dtypes(include=[\"int64\", \"float64\"]).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"integer\")  # Convert int64 → int32\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\")    # Convert float64 → float32\n",
    "    return df\n",
    "\n",
    "def load_house_data(house_path, save_dir=\"datasets/korean_processed\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    house_id = os.path.basename(house_path)\n",
    "    sub_folders = [d for d in os.listdir(house_path) if os.path.isdir(os.path.join(house_path, d))]\n",
    "\n",
    "    if not sub_folders:\n",
    "        print(f\"No sub-folder found in {house_path}\")\n",
    "        return None\n",
    "    house_sub_folder = os.path.join(house_path, sub_folders[0])\n",
    "    for timestamp_folder in sorted(os.listdir(house_sub_folder)):\n",
    "        full_timestamp_path = os.path.join(house_sub_folder, timestamp_folder)\n",
    "        print(f\"Timestamp: {timestamp_folder}\")\n",
    "        if os.path.isdir(full_timestamp_path):\n",
    "            daily_appliance_df_list = []\n",
    "            daily_total_load_df = None\n",
    "            for file in glob(os.path.join(full_timestamp_path, \"*.parquet.gzip\")):\n",
    "                appliance_name = os.path.basename(file).replace(\".parquet.gzip\", \"\")\n",
    "\n",
    "                df = pd.read_parquet(file, engine=\"pyarrow\")\n",
    "\n",
    "                # ✅ Convert timestamp correctly\n",
    "                df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"] / 1000, unit=\"s\")\n",
    "\n",
    "                # ✅ Optimize memory usage\n",
    "                df = optimize_memory(df)  \n",
    "                if \"total\" in appliance_name.lower():\n",
    "                    daily_total_load_df = df\n",
    "                else:\n",
    "                    df[\"appliance\"] = appliance_name\n",
    "                    daily_appliance_df_list.append(df)\n",
    "\n",
    "            if daily_appliance_df_list:\n",
    "                daily_appliance_df = pd.concat(daily_appliance_df_list, ignore_index=True)\n",
    "                daily_appliance_df = daily_appliance_df.pivot_table(\n",
    "                    index=\"timestamp\",\n",
    "                    columns=\"appliance\",\n",
    "                    values=[\"active_power\", \"reactive_power\"]\n",
    "                )\n",
    "                daily_appliance_df.columns = [f\"{col[1]}_{col[0]}\" for col in daily_appliance_df.columns]\n",
    "                daily_appliance_df = daily_appliance_df.reset_index()\n",
    "\n",
    "                if daily_total_load_df is not None:\n",
    "                    daily_appliance_df = daily_appliance_df.merge(daily_total_load_df, on=\"timestamp\", suffixes=(\"\", \"_total\"))\n",
    "\n",
    "                daily_appliance_df.to_parquet(f\"{save_dir}/{house_id}_{timestamp_folder}.parquet\", compression=\"gzip\")\n",
    "                print(f\"✅ Processed {house_id} for day {timestamp_folder}\")\n",
    "\n",
    "                del daily_appliance_df_list, daily_total_load_df\n",
    "                gc.collect()\n",
    "    print(f\"🎯 Finished processing {house_id}\")\n",
    "\n",
    "# def process_house(house_id):\n",
    "#     house_path = os.path.join(\"datasets/korean_extracted\", house_id)\n",
    "\n",
    "#     if os.path.isdir(house_path):\n",
    "#         print(f\"🔄 Processing {house_id}...\")\n",
    "#         return house_id, load_house_data(house_path)\n",
    "\n",
    "#     return house_id, None\n",
    "\n",
    "def process_all_houses():\n",
    "    extracted_data_dir = \"datasets/korean_extracted\"\n",
    "    processed_data_dir = \"datasets/korean_processed\"\n",
    "\n",
    "    os.makedirs(processed_data_dir, exist_ok=True) \n",
    "    houses = [d for d in os.listdir(extracted_data_dir) if os.path.isdir(os.path.join(extracted_data_dir, d))]\n",
    "    for house_id in houses:\n",
    "        house_path = os.path.join(extracted_data_dir, house_id)\n",
    "        if os.path.isdir(house_path):\n",
    "            print(f\"📊 Processing house {house_id}...\")\n",
    "\n",
    "            house_df = load_house_data(house_path)\n",
    "\n",
    "            if house_df is not None:\n",
    "                # Save each house’s data separately\n",
    "                house_df.to_parquet(f\"{processed_data_dir}/{house_id}.parquet\", compression=\"gzip\")\n",
    "                print(f\"✅ Finished processing {house_id}\")\n",
    "#     with Pool(processes=4) as pool:  # Adjust based on CPU cores\n",
    "#         results = pool.map(process_house, houses)\n",
    "\n",
    "#     all_houses_data = {house_id: df for house_id, df in results if df is not None}\n",
    "\n",
    "#     print(f\"✅ Loaded {len(all_houses_data)} houses' data in parallel!\")\n",
    "#     return all_houses_data\n",
    "# extracted_korean_path = 'datasets/korean_extracted'\n",
    "# all_houses_data = {}\n",
    "\n",
    "def resample_to_5Hz(df):\n",
    "    processed_data_dir = \"datasets/processed_data\"\n",
    "    resampled_data_dir = \"datasets/resampled_data\"\n",
    "\n",
    "    os.makedirs(resampled_data_dir, exist_ok=True)\n",
    "\n",
    "    for house_file in os.listdir(processed_data_dir):\n",
    "        if house_file.endswith(\".parquet\"):\n",
    "            house_path = os.path.join(processed_data_dir, house_file)\n",
    "            house_df = pd.read_parquet(house_path)\n",
    "\n",
    "            # Set timestamp as index and resample\n",
    "            house_df = house_df.set_index(\"timestamp\")\n",
    "            house_df_5Hz = house_df.resample(\"200ms\").mean().reset_index()\n",
    "\n",
    "            # Save resampled data\n",
    "            house_df_5Hz.to_parquet(f\"{resampled_data_dir}/{house_file}\", compression=\"gzip\")\n",
    "            print(f\"✅ Resampled {house_file} to 5Hz\")\n",
    "    # df = df.set_index(\"timestamp\")  # Ensure timestamp is the index\n",
    "    # df_5Hz = df.resample(\"200ms\").mean().reset_index()  # 5Hz resampling\n",
    "    # return df_5Hz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_dataset():\n",
    "    all_houses_data = process_all_houses()\n",
    "\n",
    "    for house_id, df in all_houses_data.items():\n",
    "        df_5Hz = resample_to_5Hz(df)\n",
    "        df_5Hz.to_parquet(f\"datasets/processed_data/{house_id}_5Hz.parquet\", compression=\"gzip\")\n",
    "        print(f\"📁 Saved downsampled data for {house_id}\")\n",
    "\n",
    "    # Merge all houses into a final dataset\n",
    "    # final_dataset = pd.concat([resample_to_5Hz(df) for df in all_houses_data.values()], ignore_index=True)\n",
    "    # final_dataset.to_parquet(\"final_microgrid_5Hz.parquet\", compression=\"gzip\")\n",
    "\n",
    "    # print(\"✅ Final dataset created and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done extracting Korean data\n",
      "Timestamp: 20161101\n",
      "✅ Processed enertalk-dataset-00 for day 20161101\n",
      "Timestamp: 20161102\n",
      "✅ Processed enertalk-dataset-00 for day 20161102\n",
      "Timestamp: 20161103\n",
      "✅ Processed enertalk-dataset-00 for day 20161103\n",
      "Timestamp: 20161104\n",
      "✅ Processed enertalk-dataset-00 for day 20161104\n",
      "Timestamp: 20161105\n",
      "✅ Processed enertalk-dataset-00 for day 20161105\n",
      "Timestamp: 20161106\n",
      "✅ Processed enertalk-dataset-00 for day 20161106\n",
      "Timestamp: 20161107\n",
      "✅ Processed enertalk-dataset-00 for day 20161107\n",
      "Timestamp: 20161109\n",
      "✅ Processed enertalk-dataset-00 for day 20161109\n",
      "Timestamp: 20161110\n",
      "✅ Processed enertalk-dataset-00 for day 20161110\n",
      "Timestamp: 20161111\n",
      "✅ Processed enertalk-dataset-00 for day 20161111\n",
      "Timestamp: 20161112\n",
      "✅ Processed enertalk-dataset-00 for day 20161112\n",
      "Timestamp: 20161113\n",
      "✅ Processed enertalk-dataset-00 for day 20161113\n",
      "Timestamp: 20161114\n",
      "✅ Processed enertalk-dataset-00 for day 20161114\n",
      "Timestamp: 20161115\n",
      "✅ Processed enertalk-dataset-00 for day 20161115\n",
      "Timestamp: 20161116\n",
      "✅ Processed enertalk-dataset-00 for day 20161116\n",
      "Timestamp: 20161117\n",
      "✅ Processed enertalk-dataset-00 for day 20161117\n",
      "Timestamp: 20161118\n",
      "✅ Processed enertalk-dataset-00 for day 20161118\n",
      "Timestamp: 20161119\n",
      "✅ Processed enertalk-dataset-00 for day 20161119\n",
      "Timestamp: 20161121\n",
      "✅ Processed enertalk-dataset-00 for day 20161121\n",
      "Timestamp: 20161122\n",
      "✅ Processed enertalk-dataset-00 for day 20161122\n",
      "Timestamp: 20161123\n",
      "✅ Processed enertalk-dataset-00 for day 20161123\n",
      "Timestamp: 20161125\n",
      "✅ Processed enertalk-dataset-00 for day 20161125\n",
      "Timestamp: 20161126\n",
      "✅ Processed enertalk-dataset-00 for day 20161126\n",
      "Timestamp: 20161128\n",
      "✅ Processed enertalk-dataset-00 for day 20161128\n",
      "Timestamp: 20161129\n",
      "✅ Processed enertalk-dataset-00 for day 20161129\n",
      "Timestamp: 20161130\n",
      "✅ Processed enertalk-dataset-00 for day 20161130\n",
      "Timestamp: 20161201\n",
      "✅ Processed enertalk-dataset-00 for day 20161201\n",
      "Timestamp: 20161202\n",
      "✅ Processed enertalk-dataset-00 for day 20161202\n",
      "Timestamp: 20161203\n",
      "✅ Processed enertalk-dataset-00 for day 20161203\n",
      "Timestamp: 20161204\n",
      "✅ Processed enertalk-dataset-00 for day 20161204\n",
      "Timestamp: 20161205\n",
      "✅ Processed enertalk-dataset-00 for day 20161205\n",
      "Timestamp: 20161206\n",
      "✅ Processed enertalk-dataset-00 for day 20161206\n",
      "Timestamp: 20161207\n",
      "✅ Processed enertalk-dataset-00 for day 20161207\n",
      "Timestamp: 20161208\n",
      "✅ Processed enertalk-dataset-00 for day 20161208\n",
      "Timestamp: 20161209\n",
      "✅ Processed enertalk-dataset-00 for day 20161209\n",
      "Timestamp: 20161210\n",
      "✅ Processed enertalk-dataset-00 for day 20161210\n",
      "Timestamp: 20161211\n",
      "✅ Processed enertalk-dataset-00 for day 20161211\n",
      "Timestamp: 20161212\n",
      "✅ Processed enertalk-dataset-00 for day 20161212\n",
      "Timestamp: 20161213\n",
      "✅ Processed enertalk-dataset-00 for day 20161213\n",
      "Timestamp: 20161214\n",
      "✅ Processed enertalk-dataset-00 for day 20161214\n",
      "Timestamp: 20161215\n",
      "✅ Processed enertalk-dataset-00 for day 20161215\n",
      "Timestamp: 20161216\n",
      "✅ Processed enertalk-dataset-00 for day 20161216\n",
      "Timestamp: 20161217\n",
      "✅ Processed enertalk-dataset-00 for day 20161217\n",
      "Timestamp: 20161218\n",
      "✅ Processed enertalk-dataset-00 for day 20161218\n",
      "Timestamp: 20161219\n",
      "✅ Processed enertalk-dataset-00 for day 20161219\n",
      "Timestamp: 20161220\n",
      "✅ Processed enertalk-dataset-00 for day 20161220\n",
      "Timestamp: 20161221\n",
      "✅ Processed enertalk-dataset-00 for day 20161221\n",
      "Timestamp: 20161222\n",
      "✅ Processed enertalk-dataset-00 for day 20161222\n",
      "Timestamp: 20161223\n",
      "✅ Processed enertalk-dataset-00 for day 20161223\n",
      "Timestamp: 20161224\n",
      "✅ Processed enertalk-dataset-00 for day 20161224\n",
      "Timestamp: 20161225\n",
      "✅ Processed enertalk-dataset-00 for day 20161225\n",
      "Timestamp: 20161226\n",
      "✅ Processed enertalk-dataset-00 for day 20161226\n",
      "Timestamp: 20161227\n",
      "✅ Processed enertalk-dataset-00 for day 20161227\n",
      "Timestamp: 20161228\n",
      "✅ Processed enertalk-dataset-00 for day 20161228\n",
      "Timestamp: 20161229\n",
      "✅ Processed enertalk-dataset-00 for day 20161229\n",
      "Timestamp: 20161230\n",
      "✅ Processed enertalk-dataset-00 for day 20161230\n",
      "Timestamp: 20170101\n",
      "✅ Processed enertalk-dataset-00 for day 20170101\n",
      "Timestamp: 20170102\n",
      "✅ Processed enertalk-dataset-00 for day 20170102\n",
      "Timestamp: 20170103\n",
      "✅ Processed enertalk-dataset-00 for day 20170103\n",
      "Timestamp: 20170104\n",
      "✅ Processed enertalk-dataset-00 for day 20170104\n",
      "Timestamp: 20170105\n",
      "✅ Processed enertalk-dataset-00 for day 20170105\n",
      "Timestamp: 20170106\n",
      "✅ Processed enertalk-dataset-00 for day 20170106\n",
      "Timestamp: 20170107\n",
      "✅ Processed enertalk-dataset-00 for day 20170107\n",
      "Timestamp: 20170108\n",
      "✅ Processed enertalk-dataset-00 for day 20170108\n",
      "Timestamp: 20170109\n",
      "✅ Processed enertalk-dataset-00 for day 20170109\n",
      "Timestamp: 20170110\n",
      "✅ Processed enertalk-dataset-00 for day 20170110\n",
      "Timestamp: 20170111\n",
      "✅ Processed enertalk-dataset-00 for day 20170111\n",
      "Timestamp: 20170112\n",
      "✅ Processed enertalk-dataset-00 for day 20170112\n",
      "Timestamp: 20170113\n",
      "✅ Processed enertalk-dataset-00 for day 20170113\n",
      "Timestamp: 20170114\n",
      "✅ Processed enertalk-dataset-00 for day 20170114\n",
      "Timestamp: 20170115\n",
      "✅ Processed enertalk-dataset-00 for day 20170115\n",
      "Timestamp: 20170116\n",
      "✅ Processed enertalk-dataset-00 for day 20170116\n",
      "Timestamp: 20170117\n",
      "✅ Processed enertalk-dataset-00 for day 20170117\n",
      "Timestamp: 20170118\n",
      "✅ Processed enertalk-dataset-00 for day 20170118\n",
      "Timestamp: 20170119\n",
      "✅ Processed enertalk-dataset-00 for day 20170119\n",
      "Timestamp: 20170120\n",
      "✅ Processed enertalk-dataset-00 for day 20170120\n",
      "Timestamp: 20170121\n",
      "✅ Processed enertalk-dataset-00 for day 20170121\n",
      "Timestamp: 20170122\n",
      "✅ Processed enertalk-dataset-00 for day 20170122\n",
      "Timestamp: 20170123\n",
      "✅ Processed enertalk-dataset-00 for day 20170123\n",
      "Timestamp: 20170124\n",
      "✅ Processed enertalk-dataset-00 for day 20170124\n",
      "Timestamp: 20170125\n",
      "✅ Processed enertalk-dataset-00 for day 20170125\n",
      "Timestamp: 20170126\n",
      "✅ Processed enertalk-dataset-00 for day 20170126\n",
      "Timestamp: 20170127\n",
      "✅ Processed enertalk-dataset-00 for day 20170127\n",
      "Timestamp: 20170128\n",
      "✅ Processed enertalk-dataset-00 for day 20170128\n",
      "Timestamp: 20170129\n",
      "✅ Processed enertalk-dataset-00 for day 20170129\n",
      "Timestamp: 20170130\n",
      "✅ Processed enertalk-dataset-00 for day 20170130\n",
      "Timestamp: 20170131\n",
      "✅ Processed enertalk-dataset-00 for day 20170131\n",
      "🎯 Finished processing enertalk-dataset-00\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "resample_to_5Hz() missing 1 required positional argument: 'df'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m extract_zips()  \u001b[38;5;66;03m# Step 1: Extract ZIPs (only if needed)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m load_house_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets/korean_extracted/enertalk-dataset-00\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Step 2: Process each house individually\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m resample_to_5Hz()\n",
      "\u001b[1;31mTypeError\u001b[0m: resample_to_5Hz() missing 1 required positional argument: 'df'"
     ]
    }
   ],
   "source": [
    "extract_zips()  # Step 1: Extract ZIPs (only if needed)\n",
    "load_house_data(\"datasets/korean_extracted/enertalk-dataset-00\")  # Step 2: Process each house individually\n",
    "resample_to_5Hz() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
